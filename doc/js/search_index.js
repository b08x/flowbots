var search_data = {"index":{"searchIndex":["api","accumulatefilteredsegmentstask","compressiontask","compressiontestassessmenttask","compressiontestevaltask","compressiontesttask","cursor","displayresultstask","exceptionagent","fileloadertask","fileobject","filtersegmentstask","finalreporttask","flowbots","apierror","agenterror","batchprocessor","cli","configurationerror","exceptionagent","exceptionhandler","filediscovery","fileloader","filenotfounderror","flowboterror","grammarprocessor","nlpprocessor","task","tasknotfounderror","textprocessingworkflow","textprocessor","textsegmentprocessor","texttaggerprocessor","texttokenizeprocessor","topicmodelprocessor","topicmodeltrainerworkflow","topicmodeltrainerworkflowtest","unifiedfileprocessingpipeline","workflowerror","workflows","flowiseapiclient","inputretrieval","jongleur","workertask","lemma","llmanalysistask","loadfileobjecttask","loadtextfilestask","logging","markdownyaml","document0","yamlfrontmatter0","yamlfrontmatter1","markdownyamlparser","microagenttask","monadicerror","nlpanalysistask","object","preprocessfileobjecttask","redisconnection","rediskeys","runrubyteststask","segment","sublayer","actions","runtestcommandaction","speechtotextaction","texttospeechaction","writefileaction","tty","markdown","converter","promptx","task","textsegmenttask","texttaggertask","texttokenizetask","tokenizesegmentstask","topic","topicmodelingtask","traintopicmodeltask","ui","box","scrollablebox","word","workflowagent","workfloworchestrator","_nt_document()","_nt_markdown_content()","_nt_newline()","_nt_yaml_front_matter()","add_agent()","add_lemma()","add_lemmas()","add_lemmas_to_textfile()","add_segment()","add_segments()","add_topics()","add_word()","add_words()","add_words_to_segment()","after_delete()","after_save()","analyze_transitivity()","call()","call()","call()","call()","classify_file()","clean_segments()","clean_segments_for_modeling()","clean_segments_for_modeling()","cleanup()","comparison_box()","complete()","completed()","configure_logger_for()","convert_p()","create_doc()","create_new_model()","create_or_fetch_file_object()","create_scrollable_box()","create_with_timestamp()","current_batch()","define_workflow()","determine_preprocessing_method()","discover_files()","discover_files()","display_boxes()","display_filtered_segments()","display_results()","display_workflows()","duration()","ensure_model_exists()","eval_result_box()","exception_box()","execute()","execute()","execute()","execute()","execute()","execute()","execute()","execute()","execute()","execute()","execute()","execute()","execute()","execute()","execute()","execute()","execute()","execute()","execute()","execute()","execute()","execute()","execute()","execute()","exit_on_failure?()","extract_main_topics()","extract_markdown_content()","extract_metadata()","extract_pdf_metadata()","extract_relevant_files()","extract_text()","extract_text_from_pdf()","extract_text_json()","extract_workflow_description()","extract_yaml_front_matter()","fail()","failed()","fallback_exception_report()","fetch_unprocessed_file_ids()","file_count()","file_types_pattern()","filter_segment_words()","filter_segment_words()","filter_segments()","find_or_create_by_path()","flush_redis_cache()","flush_redis_cache()","footer()","format_analysis()","format_exception()","format_exception_report()","format_file_info()","format_nlp_result()","format_output()","generate_analysis_prompt()","generate_exception_prompt()","get()","get_documents()","get_input_for_analysis()","get_object_attributes()","get_object_bucket()","get_object_by_name()","get_object_collections()","get_object_indexed_attributes()","get_object_references()","get_objects()","get_objects_by_collection()","get_objects_by_query()","get_objects_by_reference()","get_objects_by_regex()","get_workflows()","handle_exception()","handle_response()","header()","identify_speech_acts()","in_progress()","infer_topics()","info()","info_box()","initialize()","latest()","list_and_select()","load_components()","load_engtagger()","load_existing_model()","load_file_structure()","load_grammar()","load_model()","load_or_create_model()","load_state()","load_tasks()","load_workflows()","log_exception()","log_level()","logger()","logger_for()","main()","main_menu()","markdown_content()","multi_column_box()","new()","new()","new()","new()","new()","new()","new()","new()","new()","new()","new()","new()","new()","new()","new()","new()","new()","new()","new()","new()","new()","new()","new()","new()","new()","new()","new()","new()","new()","newline1()","newline2()","notify_exception()","parse()","parse_pdf()","pending()","perform_additional_tasks()","pos()","predict()","preprocess_file()","preprocess_json()","preprocess_markdown_yaml()","preprocess_pdf()","preprocess_plain_text()","print_boxes()","print_navigation_info()","process()","process()","process()","process()","process()","process()","process()","process_batch()","process_batch()","process_batch()","process_batch()","process_exception()","process_exception()","process_file()","process_files()","process_files()","process_single_file()","process_single_file()","process_text()","prompt()","prompt_for_file()","prompt_for_folder()","prompt_for_folder()","prompt_for_folder()","readline()","retrieve_file_metadata()","retrieve_file_object()","retrieve_file_path()","retrieve_file_path()","retrieve_filtered_words()","retrieve_input()","retrieve_input()","retrieve_input()","retrieve_input()","retrieve_input()","retrieve_input()","retrieve_input()","retrieve_input()","retrieve_input()","retrieve_input()","retrieve_input()","retrieve_input()","retrieve_input()","retrieve_nlp_result()","retrieve_segment_texts()","retrieve_segments()","retrieve_word_texts()","retrieve_word_texts()","retrieve_words()","retrieve_words()","root()","run()","run()","run()","run()","run_workflow()","save_model()","save_state()","say()","segment_array()","segment_string()","select_workflow()","set()","setup_redis()","setup_workflow()","setup_workflow()","shutdown()","side_by_side_boxes()","spinner()","splat_sort()","stop_running_workflows()","store_fileobject_id()","store_analysis_result()","store_exception_report()","store_file_data()","store_file_object_id()","store_preprocessed_data()","store_result()","store_result()","store_segments()","store_textfile_id()","store_topic_result()","store_topics()","tokenize_array()","tokenize_string()","train_model()","train_topic_model()","train_topic_model()","train_topic_model()","update_file_object()","update_segment_with_nlp_data()","update_state()","upsert_document()","version()","workflows()","write_markdown()","write_markdown_report()","write_markdown_report()","yaml_front_matter()","license","readme"],"longSearchIndex":["api","accumulatefilteredsegmentstask","compressiontask","compressiontestassessmenttask","compressiontestevaltask","compressiontesttask","cursor","displayresultstask","exceptionagent","fileloadertask","fileobject","filtersegmentstask","finalreporttask","flowbots","flowbots::apierror","flowbots::agenterror","flowbots::batchprocessor","flowbots::cli","flowbots::configurationerror","flowbots::exceptionagent","flowbots::exceptionhandler","flowbots::filediscovery","flowbots::fileloader","flowbots::filenotfounderror","flowbots::flowboterror","flowbots::grammarprocessor","flowbots::nlpprocessor","flowbots::task","flowbots::tasknotfounderror","flowbots::textprocessingworkflow","flowbots::textprocessor","flowbots::textsegmentprocessor","flowbots::texttaggerprocessor","flowbots::texttokenizeprocessor","flowbots::topicmodelprocessor","flowbots::topicmodeltrainerworkflow","flowbots::topicmodeltrainerworkflowtest","flowbots::unifiedfileprocessingpipeline","flowbots::workflowerror","flowbots::workflows","flowiseapiclient","inputretrieval","jongleur","jongleur::workertask","lemma","llmanalysistask","loadfileobjecttask","loadtextfilestask","logging","markdownyaml","markdownyaml::document0","markdownyaml::yamlfrontmatter0","markdownyaml::yamlfrontmatter1","markdownyamlparser","microagenttask","monadicerror","nlpanalysistask","object","preprocessfileobjecttask","redisconnection","rediskeys","runrubyteststask","segment","sublayer","sublayer::actions","sublayer::actions::runtestcommandaction","sublayer::actions::speechtotextaction","sublayer::actions::texttospeechaction","sublayer::actions::writefileaction","tty","tty::markdown","tty::markdown::converter","tty::promptx","task","textsegmenttask","texttaggertask","texttokenizetask","tokenizesegmentstask","topic","topicmodelingtask","traintopicmodeltask","ui","ui::box","ui::scrollablebox","word","workflowagent","workfloworchestrator","markdownyaml#_nt_document()","markdownyaml#_nt_markdown_content()","markdownyaml#_nt_newline()","markdownyaml#_nt_yaml_front_matter()","workfloworchestrator#add_agent()","fileobject#add_lemma()","fileobject#add_lemmas()","nlpanalysistask#add_lemmas_to_textfile()","fileobject#add_segment()","fileobject#add_segments()","fileobject#add_topics()","segment#add_word()","segment#add_words()","nlpanalysistask#add_words_to_segment()","fileobject#after_delete()","fileobject#after_save()","flowbots::texttaggerprocessor#analyze_transitivity()","sublayer::actions::runtestcommandaction#call()","sublayer::actions::speechtotextaction#call()","sublayer::actions::texttospeechaction#call()","sublayer::actions::writefileaction#call()","flowbots::fileloader#classify_file()","accumulatefilteredsegmentstask#clean_segments()","flowbots::topicmodeltrainerworkflow#clean_segments_for_modeling()","flowbots::topicmodeltrainerworkflowtest#clean_segments_for_modeling()","workfloworchestrator#cleanup()","ui::box#comparison_box()","task#complete()","task::completed()","logging::configure_logger_for()","tty::markdown::converter#convert_p()","flowbots::nlpprocessor#create_doc()","flowbots::topicmodelprocessor#create_new_model()","flowbots::textprocessingworkflow#create_or_fetch_file_object()","ui::scrollablebox::create_scrollable_box()","task::create_with_timestamp()","fileobject::current_batch()","workfloworchestrator#define_workflow()","preprocessfileobjecttask#determine_preprocessing_method()","flowbots::batchprocessor#discover_files()","flowbots::filediscovery::discover_files()","ui::scrollablebox::display_boxes()","filtersegmentstask#display_filtered_segments()","displayresultstask#display_results()","flowbots::workflows#display_workflows()","task#duration()","flowbots::topicmodelprocessor#ensure_model_exists()","ui::box#eval_result_box()","ui::box#exception_box()","accumulatefilteredsegmentstask#execute()","compressiontask#execute()","compressiontestassessmenttask#execute()","compressiontestevaltask#execute()","compressiontesttask#execute()","displayresultstask#execute()","fileloadertask#execute()","filtersegmentstask#execute()","finalreporttask#execute()","flowbots::task#execute()","llmanalysistask#execute()","loadfileobjecttask#execute()","loadtextfilestask#execute()","microagenttask#execute()","nlpanalysistask#execute()","preprocessfileobjecttask#execute()","runrubyteststask#execute()","task#execute()","textsegmenttask#execute()","texttaggertask#execute()","texttokenizetask#execute()","tokenizesegmentstask#execute()","topicmodelingtask#execute()","traintopicmodeltask#execute()","flowbots::cli::exit_on_failure?()","flowbots::texttaggerprocessor#extract_main_topics()","flowbots::grammarprocessor#extract_markdown_content()","preprocessfileobjecttask#extract_metadata()","preprocessfileobjecttask#extract_pdf_metadata()","flowbots::exceptionagent#extract_relevant_files()","flowbots::fileloader#extract_text()","preprocessfileobjecttask#extract_text_from_pdf()","flowbots::fileloader#extract_text_json()","flowbots::workflows#extract_workflow_description()","flowbots::grammarprocessor#extract_yaml_front_matter()","task#fail()","task::failed()","flowbots::exceptionagent#fallback_exception_report()","flowbots::textprocessingworkflow#fetch_unprocessed_file_ids()","flowbots::filediscovery::file_count()","flowbots::batchprocessor#file_types_pattern()","filtersegmentstask#filter_segment_words()","topicmodelingtask#filter_segment_words()","filtersegmentstask#filter_segments()","fileobject::find_or_create_by_path()","flowbots::topicmodeltrainerworkflowtest#flush_redis_cache()","flowbots::unifiedfileprocessingpipeline#flush_redis_cache()","ui#footer()","displayresultstask#format_analysis()","exceptionagent#format_exception()","flowbots::exceptionagent#format_exception_report()","displayresultstask#format_file_info()","llmanalysistask#format_nlp_result()","object#format_output()","llmanalysistask#generate_analysis_prompt()","flowbots::exceptionagent#generate_exception_prompt()","rediskeys::get()","topicmodelingtask#get_documents()","microagenttask#get_input_for_analysis()","object#get_object_attributes()","object#get_object_bucket()","object#get_object_by_name()","object#get_object_collections()","object#get_object_indexed_attributes()","object#get_object_references()","object#get_objects()","object#get_objects_by_collection()","object#get_objects_by_query()","object#get_objects_by_reference()","object#get_objects_by_regex()","flowbots::workflows#get_workflows()","flowbots::exceptionhandler::handle_exception()","flowiseapiclient#handle_response()","ui#header()","flowbots::texttaggerprocessor#identify_speech_acts()","task::in_progress()","flowbots::topicmodelprocessor#infer_topics()","ui#info()","ui::box#info_box()","flowbots::initialize()","fileobject::latest()","flowbots::workflows#list_and_select()","flowbots::load_components()","flowbots::texttaggerprocessor#load_engtagger()","flowbots::topicmodelprocessor#load_existing_model()","flowbots::exceptionagent#load_file_structure()","flowbots::grammarprocessor#load_grammar()","flowbots::nlpprocessor#load_model()","flowbots::topicmodelprocessor#load_or_create_model()","workflowagent#load_state()","flowbots::task::load_tasks()","flowbots::workflows::load_workflows()","flowbots::exceptionhandler::log_exception()","logging::log_level()","logging#logger()","logging::logger_for()","object#main()","ui#main_menu()","markdownyaml::document0#markdown_content()","ui::box#multi_column_box()","exceptionagent::new()","flowbots::batchprocessor::new()","flowbots::exceptionagent::new()","flowbots::fileloader::new()","flowbots::flowboterror::new()","flowbots::grammarprocessor::new()","flowbots::nlpprocessor::new()","flowbots::task::new()","flowbots::textprocessingworkflow::new()","flowbots::textprocessor::new()","flowbots::textsegmentprocessor::new()","flowbots::texttaggerprocessor::new()","flowbots::texttokenizeprocessor::new()","flowbots::topicmodelprocessor::new()","flowbots::topicmodeltrainerworkflow::new()","flowbots::topicmodeltrainerworkflowtest::new()","flowbots::unifiedfileprocessingpipeline::new()","flowbots::workflows::new()","flowiseapiclient::new()","microagenttask::new()","redisconnection::new()","sublayer::actions::runtestcommandaction::new()","sublayer::actions::speechtotextaction::new()","sublayer::actions::texttospeechaction::new()","sublayer::actions::writefileaction::new()","tty::promptx::new()","topicmodelingtask::new()","workflowagent::new()","workfloworchestrator::new()","markdownyaml::yamlfrontmatter1#newline1()","markdownyaml::yamlfrontmatter1#newline2()","flowbots::exceptionhandler::notify_exception()","flowbots::grammarprocessor#parse()","flowbots::fileloader#parse_pdf()","task::pending()","flowbots::textprocessingworkflow#perform_additional_tasks()","cursor::pos()","flowiseapiclient#predict()","preprocessfileobjecttask#preprocess_file()","preprocessfileobjecttask#preprocess_json()","preprocessfileobjecttask#preprocess_markdown_yaml()","preprocessfileobjecttask#preprocess_pdf()","preprocessfileobjecttask#preprocess_plain_text()","ui::scrollablebox::print_boxes()","ui::scrollablebox::print_navigation_info()","flowbots::nlpprocessor#process()","flowbots::textprocessor#process()","flowbots::textsegmentprocessor#process()","flowbots::texttaggerprocessor#process()","flowbots::texttokenizeprocessor#process()","flowbots::unifiedfileprocessingpipeline#process()","workflowagent#process()","flowbots::batchprocessor#process_batch()","flowbots::textprocessingworkflow#process_batch()","flowbots::topicmodeltrainerworkflowtest#process_batch()","flowbots::unifiedfileprocessingpipeline#process_batch()","exceptionagent#process_exception()","flowbots::exceptionagent#process_exception()","flowbots::unifiedfileprocessingpipeline#process_file()","flowbots::batchprocessor#process_files()","flowbots::topicmodeltrainerworkflowtest#process_files()","flowbots::textprocessingworkflow#process_single_file()","flowbots::unifiedfileprocessingpipeline#process_single_file()","flowbots::cli#process_text()","ui#prompt()","flowbots::textprocessingworkflow#prompt_for_file()","flowbots::batchprocessor#prompt_for_folder()","flowbots::topicmodeltrainerworkflow#prompt_for_folder()","flowbots::topicmodeltrainerworkflowtest#prompt_for_folder()","tty::promptx#readline()","llmanalysistask#retrieve_file_metadata()","inputretrieval#retrieve_file_object()","inputretrieval#retrieve_file_path()","loadfileobjecttask#retrieve_file_path()","topicmodelingtask#retrieve_filtered_words()","accumulatefilteredsegmentstask#retrieve_input()","displayresultstask#retrieve_input()","fileloadertask#retrieve_input()","filtersegmentstask#retrieve_input()","inputretrieval#retrieve_input()","llmanalysistask#retrieve_input()","loadtextfilestask#retrieve_input()","nlpanalysistask#retrieve_input()","task#retrieve_input()","textsegmenttask#retrieve_input()","texttaggertask#retrieve_input()","tokenizesegmentstask#retrieve_input()","topicmodelingtask#retrieve_input()","llmanalysistask#retrieve_nlp_result()","fileobject#retrieve_segment_texts()","fileobject#retrieve_segments()","fileobject#retrieve_word_texts()","segment#retrieve_word_texts()","fileobject#retrieve_words()","segment#retrieve_words()","markdownyaml#root()","flowbots::textprocessingworkflow#run()","flowbots::topicmodeltrainerworkflow#run()","flowbots::topicmodeltrainerworkflowtest#run()","flowbots::workflows#run()","workfloworchestrator#run_workflow()","flowbots::topicmodelprocessor#save_model()","workflowagent#save_state()","ui#say()","flowbots::textsegmentprocessor#segment_array()","flowbots::textsegmentprocessor#segment_string()","flowbots::workflows#select_workflow()","rediskeys::set()","flowbots::setup_redis()","flowbots::topicmodeltrainerworkflowtest#setup_workflow()","flowbots::unifiedfileprocessingpipeline#setup_workflow()","flowbots::shutdown()","ui::scrollablebox#side_by_side_boxes()","ui#spinner()","api#splat_sort()","flowbots::stop_running_workflows()","fileloadertask#store_fileobject_id()","llmanalysistask#store_analysis_result()","exceptionagent#store_exception_report()","flowbots::fileloader#store_file_data()","loadfileobjecttask#store_file_object_id()","preprocessfileobjecttask#store_preprocessed_data()","microagenttask#store_result()","texttaggertask#store_result()","textsegmenttask#store_segments()","loadtextfilestask#store_textfile_id()","topicmodelingtask#store_topic_result()","topicmodelingtask#store_topics()","flowbots::texttokenizeprocessor#tokenize_array()","flowbots::texttokenizeprocessor#tokenize_string()","flowbots::topicmodelprocessor#train_model()","flowbots::cli#train_topic_model()","flowbots::topicmodeltrainerworkflow#train_topic_model()","flowbots::topicmodeltrainerworkflowtest#train_topic_model()","accumulatefilteredsegmentstask#update_file_object()","nlpanalysistask#update_segment_with_nlp_data()","workflowagent#update_state()","flowiseapiclient#upsert_document()","flowbots::cli#version()","flowbots::cli#workflows()","llmanalysistask#write_markdown()","flowbots::exceptionagent#write_markdown_report()","llmanalysistask#write_markdown_report()","markdownyaml::document0#yaml_front_matter()","",""],"info":[["API","","API.html","","<p>… (Ohm model definitions from OhmModels.rb) …\n"],["AccumulateFilteredSegmentsTask","","AccumulateFilteredSegmentsTask.html","","<p>Task to accumulate filtered segments from all processed files.\n"],["CompressionTask","","CompressionTask.html","","<p>This task compresses a prompt using a WorkflowAgent.\n"],["CompressionTestAssessmentTask","","CompressionTestAssessmentTask.html","","<p>This task assesses a compression test evaluation using a WorkflowAgent.\n"],["CompressionTestEvalTask","","CompressionTestEvalTask.html","","<p>This task evaluates a compression test design using a WorkflowAgent.\n"],["CompressionTestTask","","CompressionTestTask.html","","<p>This task designs a test for a compressed prompt using a WorkflowAgent.\n"],["Cursor","","Cursor.html","",""],["DisplayResultsTask","","DisplayResultsTask.html","","<p>This task displays the results of the text processing workflow.\n"],["ExceptionAgent","","ExceptionAgent.html","",""],["FileLoaderTask","","FileLoaderTask.html","","<p>This task loads a text file and stores its ID in Redis.\n"],["FileObject","","FileObject.html","",""],["FilterSegmentsTask","","FilterSegmentsTask.html","","<p>lib/tasks/filter_segments_task.rb\n"],["FinalReportTask","","FinalReportTask.html","","<p>This task generates a final report using a WorkflowAgent.\n"],["Flowbots","","Flowbots.html","","<p>Module for Flowbots application.\n"],["Flowbots::APIError","","Flowbots/APIError.html","","<p>Error raised when there is a problem with an API call.\n"],["Flowbots::AgentError","","Flowbots/AgentError.html","","<p>Error raised when there is a problem with an agent.\n"],["Flowbots::BatchProcessor","","Flowbots/BatchProcessor.html","","<p>The <code>BatchProcessor</code> class provides a mechanism for processing files in batches.\nIt is particularly useful ...\n"],["Flowbots::CLI","","Flowbots/CLI.html","","<p>This class provides a command-line interface (CLI) for interacting with the Flowbots application.\n"],["Flowbots::ConfigurationError","","Flowbots/ConfigurationError.html","","<p>Error raised when there is a problem with the configuration.\n"],["Flowbots::ExceptionAgent","","Flowbots/ExceptionAgent.html","","<p>This class handles exceptions in the Flowbots application.\n"],["Flowbots::ExceptionHandler","","Flowbots/ExceptionHandler.html","","<p>This class handles exceptions in the Flowbots application.\n"],["Flowbots::FileDiscovery","","Flowbots/FileDiscovery.html","","<p>This module provides file discovery utilities for Flowbots.\n"],["Flowbots::FileLoader","","Flowbots/FileLoader.html","","<p>This class handles loading and processing text files.\n"],["Flowbots::FileNotFoundError","","Flowbots/FileNotFoundError.html","","<p>Custom error class for workflow file not found.\n"],["Flowbots::FlowbotError","","Flowbots/FlowbotError.html","","<p>Base class for all Flowbots errors.\n"],["Flowbots::GrammarProcessor","","Flowbots/GrammarProcessor.html","","<p>This class handles parsing text using a specified grammar.\n"],["Flowbots::NLPProcessor","","Flowbots/NLPProcessor.html","","<p>This class provides functionality for performing natural language processing (NLP) analysis on text. …\n"],["Flowbots::Task","","Flowbots/Task.html","","<p>This module encapsulates tasks used in Flowbots workflows.\n"],["Flowbots::TaskNotFoundError","","Flowbots/TaskNotFoundError.html","","<p>Custom error class for task not found.\n"],["Flowbots::TextProcessingWorkflow","","Flowbots/TextProcessingWorkflow.html","","<p>This class defines a workflow for processing text files, either individually or in batch mode.\nIt utilizes ...\n"],["Flowbots::TextProcessor","","Flowbots/TextProcessor.html","","<p>This class provides a base class for text processors in the Flowbots application.\n"],["Flowbots::TextSegmentProcessor","","Flowbots/TextSegmentProcessor.html","","<p>This class provides functionality for segmenting text into smaller units.\n"],["Flowbots::TextTaggerProcessor","","Flowbots/TextTaggerProcessor.html","","<p>This class provides functionality for tagging text using the EngTagger library.\n"],["Flowbots::TextTokenizeProcessor","","Flowbots/TextTokenizeProcessor.html","","<p>This class provides functionality for tokenizing text.\n"],["Flowbots::TopicModelProcessor","","Flowbots/TopicModelProcessor.html","","<p>This class provides functionality for processing text using a topic model.\n"],["Flowbots::TopicModelTrainerWorkflow","","Flowbots/TopicModelTrainerWorkflow.html","","<p>This class defines a workflow for training a topic model using a collection of text files.\nIt utilizes ...\n"],["Flowbots::TopicModelTrainerWorkflowtest","","Flowbots/TopicModelTrainerWorkflowtest.html","",""],["Flowbots::UnifiedFileProcessingPipeline","","Flowbots/UnifiedFileProcessingPipeline.html","","<p>This class defines a pipeline for processing files, either individually or in batches.\nIt utilizes a ...\n"],["Flowbots::WorkflowError","","Flowbots/WorkflowError.html","","<p>Error raised when there is a problem with a workflow.\n"],["Flowbots::Workflows","","Flowbots/Workflows.html","","<p>This class manages workflows in the Flowbots application.\n"],["FlowiseApiClient","","FlowiseApiClient.html","","<p>This class provides an interface for interacting with the Flowise API.\n"],["InputRetrieval","","InputRetrieval.html","","<p>Module for retrieving input data.\n"],["Jongleur","","Jongleur.html","",""],["Jongleur::WorkerTask","","Jongleur/WorkerTask.html","","<p>Define a Redis connection for Jongleur::WorkerTask\n<p>Jongleur::WorkerTask is a class that defines a task …\n"],["Lemma","","Lemma.html","",""],["LlmAnalysisTask","","LlmAnalysisTask.html","","<p>This task performs LLM analysis on a text file using a pre-trained model.\n<p>This task performs LLM analysis …\n"],["LoadFileObjectTask","","LoadFileObjectTask.html","","<p>Task to load a FileObject based on a file path stored in Redis.\n"],["LoadTextFilesTask","","LoadTextFilesTask.html","","<p>Task to load text files and store their IDs in Redis.\n"],["Logging","","Logging.html","",""],["MarkdownYaml","","MarkdownYaml.html","","<p>Autogenerated from a Treetop grammar. Edits may be lost.\n"],["MarkdownYaml::Document0","","MarkdownYaml/Document0.html","","<p>The Document node represents the entire document structure.\nIt contains the YAML front matter and the ...\n"],["MarkdownYaml::YamlFrontMatter0","","MarkdownYaml/YamlFrontMatter0.html","","<p>The YamlFrontMatter node represents the YAML front matter section.\n"],["MarkdownYaml::YamlFrontMatter1","","MarkdownYaml/YamlFrontMatter1.html","","<p>The YamlFrontMatter node represents the YAML front matter section.\n"],["MarkdownYamlParser","","MarkdownYamlParser.html","","<p>The MarkdownYamlParser class is responsible for parsing the Markdown YAML grammar.\n"],["MicroAgentTask","","MicroAgentTask.html","",""],["MonadicError","","MonadicError.html","",""],["NlpAnalysisTask","","NlpAnalysisTask.html","","<p>This task performs natural language processing (NLP) analysis on the segments of a text file.\n"],["Object","","Object.html","",""],["PreprocessFileObjectTask","","PreprocessFileObjectTask.html","","<p>Task to preprocess a FileObject.\n"],["RedisConnection","","RedisConnection.html","","<p>Class to manage Redis connection.\n"],["RedisKeys","","RedisKeys.html","","<p>Module for managing Redis keys used in the Flowbots application.\n"],["RunRubyTestsTask","","RunRubyTestsTask.html","","<p>This task runs Ruby tests from a file.\n"],["Segment","","Segment.html","",""],["Sublayer","","Sublayer.html","","<p>blueprints.sublayer.com/blueprints/70562717-70c5-4406-a792-358d169f9f0b\n"],["Sublayer::Actions","","Sublayer/Actions.html","",""],["Sublayer::Actions::RunTestCommandAction","","Sublayer/Actions/RunTestCommandAction.html","",""],["Sublayer::Actions::SpeechToTextAction","","Sublayer/Actions/SpeechToTextAction.html","",""],["Sublayer::Actions::TextToSpeechAction","","Sublayer/Actions/TextToSpeechAction.html","",""],["Sublayer::Actions::WriteFileAction","","Sublayer/Actions/WriteFileAction.html","",""],["TTY","","TTY.html","",""],["TTY::Markdown","","TTY/Markdown.html","",""],["TTY::Markdown::Converter","","TTY/Markdown/Converter.html","","<p>Converts a Kramdown::Document tree to a terminal friendly output\n"],["TTY::PromptX","","TTY/PromptX.html","",""],["Task","","Task.html","",""],["TextSegmentTask","","TextSegmentTask.html","","<p>This task segments the text content of a Textfile into smaller units.\n"],["TextTaggerTask","","TextTaggerTask.html","","<p>This class performs text tagging on a given text.\n"],["TextTokenizeTask","","TextTokenizeTask.html","","<p>This task tokenizes the segments of a text file.\n"],["TokenizeSegmentsTask","","TokenizeSegmentsTask.html","","<p>This task tokenizes the segments of a text file.\n"],["Topic","","Topic.html","",""],["TopicModelingTask","","TopicModelingTask.html","","<p>This task performs topic modeling on a text file using a pre-trained model.\n"],["TrainTopicModelTask","","TrainTopicModelTask.html","","<p>This task trains a topic model using filtered segments from multiple batches.\n"],["UI","","UI.html","","<p>This module provides user interface (UI) elements and functions for the Flowbots application.\n<p>This module …\n"],["UI::Box","","UI/Box.html","","<p>This module provides methods for creating and displaying boxes in the UI.\n"],["UI::ScrollableBox","","UI/ScrollableBox.html","","<p>This module provides methods for creating and displaying scrollable boxes in the UI.\n"],["Word","","Word.html","",""],["WorkflowAgent","","WorkflowAgent.html","","<p>This class represents an agent in a workflow.\nClass representing an individual agent within a workflow ...\n"],["WorkflowOrchestrator","","WorkflowOrchestrator.html","","<p>Orchestrates the execution of workflows in the Flowbots application.\n<p>The WorkflowOrchestrator is responsible …\n"],["_nt_document","MarkdownYaml","MarkdownYaml.html#method-i-_nt_document","()","<p>Parses the document node.\n<p>@return [Treetop::Runtime::SyntaxNode] The parsed document node.\n"],["_nt_markdown_content","MarkdownYaml","MarkdownYaml.html#method-i-_nt_markdown_content","()","<p>Parses the Markdown content node.\n<p>@return [Treetop::Runtime::SyntaxNode] The parsed Markdown content node. …\n"],["_nt_newline","MarkdownYaml","MarkdownYaml.html#method-i-_nt_newline","()","<p>Parses the newline node.\n<p>@return [Treetop::Runtime::SyntaxNode] The parsed newline node.\n"],["_nt_yaml_front_matter","MarkdownYaml","MarkdownYaml.html#method-i-_nt_yaml_front_matter","()","<p>Parses the YAML front matter node.\n<p>@return [Treetop::Runtime::SyntaxNode] The parsed YAML front matter …\n"],["add_agent","WorkflowOrchestrator","WorkflowOrchestrator.html#method-i-add_agent","(role, cartridge_file, author: \"@b08x\")","<p>Adds an agent to the orchestrator.\n<p>@param role [String] The role of the agent in the workflow.\n@param ...\n"],["add_lemma","FileObject","FileObject.html#method-i-add_lemma","(lemma_data)",""],["add_lemmas","FileObject","FileObject.html#method-i-add_lemmas","(lemmas_data)",""],["add_lemmas_to_textfile","NlpAnalysisTask","NlpAnalysisTask.html#method-i-add_lemmas_to_textfile","(textfile, lemma_counts)","<p>Adds lemmas to a FileObject.\n<p>Converts the lemma counts hash to an array of lemma data and adds it to the …\n"],["add_segment","FileObject","FileObject.html#method-i-add_segment","(text)",""],["add_segments","FileObject","FileObject.html#method-i-add_segments","(new_segments)",""],["add_topics","FileObject","FileObject.html#method-i-add_topics","(new_topics)",""],["add_word","Segment","Segment.html#method-i-add_word","(word_data)",""],["add_words","Segment","Segment.html#method-i-add_words","(new_words)",""],["add_words_to_segment","NlpAnalysisTask","NlpAnalysisTask.html#method-i-add_words_to_segment","(segment, processed_tokens)","<p>Adds processed words to a segment.\n<p>Extracts word information from the processed tokens and adds it to …\n"],["after_delete","FileObject","FileObject.html#method-i-after_delete","()",""],["after_save","FileObject","FileObject.html#method-i-after_save","()",""],["analyze_transitivity","Flowbots::TextTaggerProcessor","Flowbots/TextTaggerProcessor.html#method-i-analyze_transitivity","(text)","<p>Analyzes the transitivity of sentences in the given text.\n<p>@param text [String] The text to analyze.\n<p>@return …\n"],["call","Sublayer::Actions::RunTestCommandAction","Sublayer/Actions/RunTestCommandAction.html#method-i-call","()",""],["call","Sublayer::Actions::SpeechToTextAction","Sublayer/Actions/SpeechToTextAction.html#method-i-call","()",""],["call","Sublayer::Actions::TextToSpeechAction","Sublayer/Actions/TextToSpeechAction.html#method-i-call","()",""],["call","Sublayer::Actions::WriteFileAction","Sublayer/Actions/WriteFileAction.html#method-i-call","()","<p>Writes the contents to the file in binary mode\n@return [void]\n"],["classify_file","Flowbots::FileLoader","Flowbots/FileLoader.html#method-i-classify_file","(file_path)","<p>Classifies the file type based on its MIME type.\n<p>@param file_path [String] The path to the file.\n<p>@return …\n"],["clean_segments","AccumulateFilteredSegmentsTask","AccumulateFilteredSegmentsTask.html#method-i-clean_segments","(segments)","<p>Cleans the given segments by removing unwanted segments and words.\n<p>@param segments [Array&lt;Array&lt;String&gt;&gt;] …\n"],["clean_segments_for_modeling","Flowbots::TopicModelTrainerWorkflow","Flowbots/TopicModelTrainerWorkflow.html#method-i-clean_segments_for_modeling","(segments)","<p>Cleans the segments for topic modeling by removing unwanted segments and words.\n<p>@param segments [Array&lt;Array&lt;String&gt;&gt;] …\n"],["clean_segments_for_modeling","Flowbots::TopicModelTrainerWorkflowtest","Flowbots/TopicModelTrainerWorkflowtest.html#method-i-clean_segments_for_modeling","(segments)",""],["cleanup","WorkflowOrchestrator","WorkflowOrchestrator.html#method-i-cleanup","()","<p>Performs cleanup operations for the workflow.\n<p>This method is called after the workflow has finished or …\n"],["comparison_box","UI::Box","UI/Box.html#method-i-comparison_box","(text1, text2, title1: \"Text 1\", title2: \"Text 2\")","<p>Creates a box containing two texts side-by-side for comparison.\n<p>@param text1 [String] The first text to …\n"],["complete","Task","Task.html#method-i-complete","(result=nil)",""],["completed","Task","Task.html#method-c-completed","()",""],["configure_logger_for","Logging","Logging.html#method-c-configure_logger_for","(_classname, _methodname)","<p>Configures a logger for the specified class and method.\n<p>@param &lt;em&gt;classname [String] The name of …\n"],["convert_p","TTY::Markdown::Converter","TTY/Markdown/Converter.html#method-i-convert_p","(ell, opts)",""],["create_doc","Flowbots::NLPProcessor","Flowbots/NLPProcessor.html#method-i-create_doc","(segment)","<p>Creates a Spacy::Doc object from the given segment’s tokens.\n<p>@param segment [Segment] The Segment …\n"],["create_new_model","Flowbots::TopicModelProcessor","Flowbots/TopicModelProcessor.html#method-i-create_new_model","()","<p>Creates a new topic model with the specified parameters.\n<p>@return [void]\n"],["create_or_fetch_file_object","Flowbots::TextProcessingWorkflow","Flowbots/TextProcessingWorkflow.html#method-i-create_or_fetch_file_object","(file_path)","<p>Creates or fetches a FileObject for the given file path.\n<p>@param file_path [String, Hash] The path to the …\n"],["create_scrollable_box","UI::ScrollableBox","UI/ScrollableBox.html#method-c-create_scrollable_box","(text, width, height, title)","<p>Creates a scrollable box data structure.\n<p>@param text [String] The text to display in the box.\n@param width ...\n"],["create_with_timestamp","Task","Task.html#method-c-create_with_timestamp","(attributes={})",""],["current_batch","FileObject","FileObject.html#method-c-current_batch","()",""],["define_workflow","WorkflowOrchestrator","WorkflowOrchestrator.html#method-i-define_workflow","(workflow_definition)","<p>Defines the workflow structure using a task graph.\n<p>The workflow definition is a hash that outlines the …\n"],["determine_preprocessing_method","PreprocessFileObjectTask","PreprocessFileObjectTask.html#method-i-determine_preprocessing_method","(file_object)","<p>Determines the preprocessing method based on the file extension.\n<p>@param file_object [FileObject] The  …\n"],["discover_files","Flowbots::BatchProcessor","Flowbots/BatchProcessor.html#method-i-discover_files","()","<p>Discovers all files within the input folder that match the specified file types.\n<p>@return [Array&lt;String&gt;] …\n"],["discover_files","Flowbots::FileDiscovery","Flowbots/FileDiscovery.html#method-c-discover_files","(directory)","<p>Discovers files in the given directory and groups them by type.\n<p>@param directory [String] The directory …\n"],["display_boxes","UI::ScrollableBox","UI/ScrollableBox.html#method-c-display_boxes","(box1, box2, box_height)","<p>Displays the scrollable boxes and handles user navigation.\n<p>@param box1 [Hash] The data for the first box. …\n"],["display_filtered_segments","FilterSegmentsTask","FilterSegmentsTask.html#method-i-display_filtered_segments","(filtered_segments)",""],["display_results","DisplayResultsTask","DisplayResultsTask.html#method-i-display_results","(textfile, analysis_result)","<p>Displays the results of the text processing workflow.\n<p>@param textfile [Textfile] The processed Textfile …\n"],["display_workflows","Flowbots::Workflows","Flowbots/Workflows.html#method-i-display_workflows","(workflows)","<p>Displays a list of available workflows in a table format.\n<p>@param workflows [Array&lt;Array(String, String …\n"],["duration","Task","Task.html#method-i-duration","()",""],["ensure_model_exists","Flowbots::TopicModelProcessor","Flowbots/TopicModelProcessor.html#method-i-ensure_model_exists","()","<p>Ensures that the topic model exists, loading or creating it if necessary.\n<p>@return [void]\n"],["eval_result_box","UI::Box","UI/Box.html#method-i-eval_result_box","(result, title: \"Evaluation Result\")","<p>Creates a box displaying the evaluation result with a success style.\n<p>@param result [String] The evaluation …\n"],["exception_box","UI::Box","UI/Box.html#method-i-exception_box","(message)","<p>Creates a box displaying an exception message with an error style.\n<p>@param message [String] The exception …\n"],["execute","AccumulateFilteredSegmentsTask","AccumulateFilteredSegmentsTask.html#method-i-execute","()","<p>Executes the task to accumulate and clean filtered segments.\n<p>Retrieves filtered segments from Redis, cleans …\n"],["execute","CompressionTask","CompressionTask.html#method-i-execute","()","<p>Executes the task.\n<p>@return [void]\n@raises [StandardError] If an error occurs during the task execution. ...\n"],["execute","CompressionTestAssessmentTask","CompressionTestAssessmentTask.html#method-i-execute","()","<p>Executes the task.\n<p>@return [void]\n@raises [StandardError] If an error occurs during the task execution. ...\n"],["execute","CompressionTestEvalTask","CompressionTestEvalTask.html#method-i-execute","()","<p>Executes the task.\n<p>@return [void]\n@raises [StandardError] If an error occurs during the task execution. ...\n"],["execute","CompressionTestTask","CompressionTestTask.html#method-i-execute","()","<p>Executes the task.\n<p>@return [void]\n@raises [StandardError] If an error occurs during the task execution. ...\n"],["execute","DisplayResultsTask","DisplayResultsTask.html#method-i-execute","()","<p>Executes the task to display the results of the text processing workflow.\n<p>Retrieves the processed Textfile …\n"],["execute","FileLoaderTask","FileLoaderTask.html#method-i-execute","()","<p>Executes the task to load a FileObject and store its ID in Redis.\n<p>Retrieves the input file path, processes …\n"],["execute","FilterSegmentsTask","FilterSegmentsTask.html#method-i-execute","()",""],["execute","FinalReportTask","FinalReportTask.html#method-i-execute","()","<p>Executes the task.\n<p>@return [void]\n@raises [StandardError] If an error occurs during the task execution. ...\n"],["execute","Flowbots::Task","Flowbots/Task.html#method-i-execute","()","<p>Executes the task.\n<p>This method must be implemented in subclasses.\n<p>@return [void]\n@raise [NotImplementedError] ...\n"],["execute","LlmAnalysisTask","LlmAnalysisTask.html#method-i-execute","()","<p>Executes the task.\n<p>@return [void]\n"],["execute","LoadFileObjectTask","LoadFileObjectTask.html#method-i-execute","()","<p>Executes the task to load a FileObject.\n<p>Retrieves the file path from Redis, finds or creates a FileObject …\n"],["execute","LoadTextFilesTask","LoadTextFilesTask.html#method-i-execute","()","<p>Executes the task to load a text file using the Flowbots::FileLoader.\n<p>Retrieves the file path from Redis …\n"],["execute","MicroAgentTask","MicroAgentTask.html#method-i-execute","()",""],["execute","NlpAnalysisTask","NlpAnalysisTask.html#method-i-execute","()","<p>Executes the task.\n<p>Retrieves the FileObject from Redis, processes each segment using the NLPProcessor …\n"],["execute","PreprocessFileObjectTask","PreprocessFileObjectTask.html#method-i-execute","()","<p>Executes the task to preprocess a FileObject.\n<p>Retrieves the FileObject from Redis, determines the appropriate …\n"],["execute","RunRubyTestsTask","RunRubyTestsTask.html#method-i-execute","()","<p>Executes the task.\n<p>@return [void]\n@raises [StandardError] If an error occurs during the task execution. ...\n"],["execute","Task","Task.html#method-i-execute","()",""],["execute","TextSegmentTask","TextSegmentTask.html#method-i-execute","()","<p>Executes the task to segment the text content of a FileObject.\n<p>Retrieves the FileObject from Redis, extracts …\n"],["execute","TextTaggerTask","TextTaggerTask.html#method-i-execute","()","<p>Executes the text tagging task.\n<p>Retrieves the FileObject from Redis, extracts its preprocessed content, …\n"],["execute","TextTokenizeTask","TextTokenizeTask.html#method-i-execute","()","<p>Executes the task.\n<p>@return [void]\n"],["execute","TokenizeSegmentsTask","TokenizeSegmentsTask.html#method-i-execute","()","<p>Executes the task to tokenize the segments of a FileObject.\n<p>Retrieves the FileObject from Redis, tokenizes …\n"],["execute","TopicModelingTask","TopicModelingTask.html#method-i-execute","()",""],["execute","TrainTopicModelTask","TrainTopicModelTask.html#method-i-execute","()","<p>Executes the task to train a topic model using accumulated filtered segments.\n<p>Retrieves the current batch …\n"],["exit_on_failure?","Flowbots::CLI","Flowbots/CLI.html#method-c-exit_on_failure-3F","()","<p>Defines whether the CLI should exit with a non-zero status code when an error occurs.\n<p>@return [Boolean] …\n"],["extract_main_topics","Flowbots::TextTaggerProcessor","Flowbots/TextTaggerProcessor.html#method-i-extract_main_topics","(text, limit=5)","<p>Extracts the main topics from the given text.\n<p>@param text [String] The text to extract topics from.\n@param ...\n"],["extract_markdown_content","Flowbots::GrammarProcessor","Flowbots/GrammarProcessor.html#method-i-extract_markdown_content","(parse_result)","<p>Extracts the Markdown content from the parse result.\n<p>@param parse_result [Treetop::Runtime::SyntaxNode] …\n"],["extract_metadata","PreprocessFileObjectTask","PreprocessFileObjectTask.html#method-i-extract_metadata","(yaml_front_matter)","<p>Extracts metadata from YAML front matter.\n<p>@param yaml_front_matter [String] The YAML front matter string. …\n"],["extract_pdf_metadata","PreprocessFileObjectTask","PreprocessFileObjectTask.html#method-i-extract_pdf_metadata","(pdf_path)","<p>Extracts metadata from a PDF file.\n<p>@param pdf_path [String] The path to the PDF file.\n<p>@return [Hash] The …\n"],["extract_relevant_files","Flowbots::ExceptionAgent","Flowbots/ExceptionAgent.html#method-i-extract_relevant_files","(exception)","<p>Extracts relevant files from the exception backtrace.\n<p>@param exception [Exception] The exception object. …\n"],["extract_text","Flowbots::FileLoader","Flowbots/FileLoader.html#method-i-extract_text","(file_type, file_path)","<p>Extracts the text content from a file based on its type.\n<p>@param file_type [Symbol] The file type.\n@param ...\n"],["extract_text_from_pdf","PreprocessFileObjectTask","PreprocessFileObjectTask.html#method-i-extract_text_from_pdf","(pdf_path)","<p>Extracts text content from a PDF file.\n<p>@param pdf_path [String] The path to the PDF file.\n<p>@return [String] …\n"],["extract_text_json","Flowbots::FileLoader","Flowbots/FileLoader.html#method-i-extract_text_json","(file_path)",""],["extract_workflow_description","Flowbots::Workflows","Flowbots/Workflows.html#method-i-extract_workflow_description","(file)","<p>Extracts the description of a workflow from its file.\nThe description is assumed to be the first line ...\n"],["extract_yaml_front_matter","Flowbots::GrammarProcessor","Flowbots/GrammarProcessor.html#method-i-extract_yaml_front_matter","(parse_result)","<p>Extracts the YAML front matter from the parse result.\n<p>@param parse_result [Treetop::Runtime::SyntaxNode] …\n"],["fail","Task","Task.html#method-i-fail","(error_message)",""],["failed","Task","Task.html#method-c-failed","()",""],["fallback_exception_report","Flowbots::ExceptionAgent","Flowbots/ExceptionAgent.html#method-i-fallback_exception_report","(exception_details)","<p>Generates a fallback exception report if the agent fails to generate a report.\n<p>@param exception_details …\n"],["fetch_unprocessed_file_ids","Flowbots::TextProcessingWorkflow","Flowbots/TextProcessingWorkflow.html#method-i-fetch_unprocessed_file_ids","()","<p>Fetches the IDs of unprocessed files.\n<p>@return [Array&lt;Integer&gt;] An array of unprocessed file IDs …\n"],["file_count","Flowbots::FileDiscovery","Flowbots/FileDiscovery.html#method-c-file_count","(files)","<p>Counts the number of files for each file type.\n<p>@param files [Hash] A hash where keys are file types and …\n"],["file_types_pattern","Flowbots::BatchProcessor","Flowbots/BatchProcessor.html#method-i-file_types_pattern","()","<p>Constructs a regular expression pattern from the <code>file_types</code> array.\n<p>@return [String] The regular expression …\n"],["filter_segment_words","FilterSegmentsTask","FilterSegmentsTask.html#method-i-filter_segment_words","(segment)",""],["filter_segment_words","TopicModelingTask","TopicModelingTask.html#method-i-filter_segment_words","(segment)","<p>Filters words from a segment based on their POS tags.\n<p>@param segment [Segment] The segment to filter words …\n"],["filter_segments","FilterSegmentsTask","FilterSegmentsTask.html#method-i-filter_segments","(file_object)",""],["find_or_create_by_path","FileObject","FileObject.html#method-c-find_or_create_by_path","(file_path, attributes={})",""],["flush_redis_cache","Flowbots::TopicModelTrainerWorkflowtest","Flowbots/TopicModelTrainerWorkflowtest.html#method-i-flush_redis_cache","()",""],["flush_redis_cache","Flowbots::UnifiedFileProcessingPipeline","Flowbots/UnifiedFileProcessingPipeline.html#method-i-flush_redis_cache","()","<p>Flushes the Redis cache.\n<p>@return [void]\n"],["footer","UI","UI.html#method-i-footer","()","<p>Displays the Flowbots footer in a framed box.\n<p>@return [void]\n"],["format_analysis","DisplayResultsTask","DisplayResultsTask.html#method-i-format_analysis","(analysis_result)","<p>Formats the analysis results for display.\n<p>@param analysis_result [String, Hash] The LLM analysis results. …\n"],["format_exception","ExceptionAgent","ExceptionAgent.html#method-i-format_exception","(exception)",""],["format_exception_report","Flowbots::ExceptionAgent","Flowbots/ExceptionAgent.html#method-i-format_exception_report","(agent_response, exception_details)","<p>Formats the exception report based on the agent’s response.\n<p>@param agent_response [String] The response …\n"],["format_file_info","DisplayResultsTask","DisplayResultsTask.html#method-i-format_file_info","(textfile)","<p>Formats the file information for display.\n<p>@param textfile [Textfile] The processed Textfile object.\n<p>@return …\n"],["format_nlp_result","LlmAnalysisTask","LlmAnalysisTask.html#method-i-format_nlp_result","(nlp_result)","<p>Formats the NLP results for display in the prompt.\n<p>@param nlp_result [Array] The NLP results for the segments …\n"],["format_output","Object","Object.html#method-i-format_output","(objects)",""],["generate_analysis_prompt","LlmAnalysisTask","LlmAnalysisTask.html#method-i-generate_analysis_prompt","(textfile, content, metadata, nlp_result)","\n<pre>Please structure your response in a clear, concise manner. Thank you!</pre>\n<p>PROMPT\nend\n"],["generate_exception_prompt","Flowbots::ExceptionAgent","Flowbots/ExceptionAgent.html#method-i-generate_exception_prompt","(exception_details)","<p>Generates a prompt for the exception handler agent.\n<p>@param exception_details [Hash] A hash containing …\n"],["get","RedisKeys","RedisKeys.html#method-c-get","(key)","<p>Retrieves the value associated with the given key from Redis.\n<p>@param key [String] The Redis key.\n@return ...\n"],["get_documents","TopicModelingTask","TopicModelingTask.html#method-i-get_documents","()",""],["get_input_for_analysis","MicroAgentTask","MicroAgentTask.html#method-i-get_input_for_analysis","()",""],["get_object_attributes","Object","Object.html#method-i-get_object_attributes","(object_bucket)",""],["get_object_bucket","Object","Object.html#method-i-get_object_bucket","(object_bucket)",""],["get_object_by_name","Object","Object.html#method-i-get_object_by_name","(object_bucket, object_name)",""],["get_object_collections","Object","Object.html#method-i-get_object_collections","(object_bucket)",""],["get_object_indexed_attributes","Object","Object.html#method-i-get_object_indexed_attributes","(object_bucket)",""],["get_object_references","Object","Object.html#method-i-get_object_references","(object_bucket)",""],["get_objects","Object","Object.html#method-i-get_objects","(object_bucket, object_name, query=nil)",""],["get_objects_by_collection","Object","Object.html#method-i-get_objects_by_collection","(object_bucket, collection_name)",""],["get_objects_by_query","Object","Object.html#method-i-get_objects_by_query","(object_bucket, query)",""],["get_objects_by_reference","Object","Object.html#method-i-get_objects_by_reference","(object_bucket, reference_name, reference_value)",""],["get_objects_by_regex","Object","Object.html#method-i-get_objects_by_regex","(object_bucket, regex)",""],["get_workflows","Flowbots::Workflows","Flowbots/Workflows.html#method-i-get_workflows","()","<p>Retrieves a list of available workflows from the WORKFLOW_DIR directory.\n<p>@return [Array&lt;Array(String …\n"],["handle_exception","Flowbots::ExceptionHandler","Flowbots/ExceptionHandler.html#method-c-handle_exception","(classname=nil, exception)","<p>Handles an exception by generating a report and notifying relevant parties.\n<p>@param classname [String] …\n"],["handle_response","FlowiseApiClient","FlowiseApiClient.html#method-i-handle_response","(response)","<p>Handles the response from the Flowise API.\n<p>@param response [Faraday::Response] The response from the  …\n"],["header","UI","UI.html#method-i-header","()","<p>Displays the Flowbots header in a framed box.\n<p>@return [void]\n"],["identify_speech_acts","Flowbots::TextTaggerProcessor","Flowbots/TextTaggerProcessor.html#method-i-identify_speech_acts","(text)","<p>Identifies the speech acts in the given text.\n<p>@param text [String] The text to analyze.\n<p>@return [Array] …\n"],["in_progress","Task","Task.html#method-c-in_progress","()",""],["infer_topics","Flowbots::TopicModelProcessor","Flowbots/TopicModelProcessor.html#method-i-infer_topics","(document)","<p>Infers the topics for a given document.\n<p>@param document [String] The document to infer topics for.\n<p>@return …\n"],["info","UI","UI.html#method-i-info","(text)","<p>Displays an information message in a framed box.\n<p>@param text [String] The text to display in the info …\n"],["info_box","UI::Box","UI/Box.html#method-i-info_box","(message, title: \"Info\")","<p>Creates a box displaying an information message with an info style.\n<p>@param message [String] The information …\n"],["initialize","Flowbots","Flowbots.html#method-c-initialize","()","<p>Initializes the Flowbots application.\n<p>@return [void]\n"],["latest","FileObject","FileObject.html#method-c-latest","(limit=nil)",""],["list_and_select","Flowbots::Workflows","Flowbots/Workflows.html#method-i-list_and_select","()","<p>Lists available workflows and allows the user to select one.\n<p>@return [String, nil] The name of the selected …\n"],["load_components","Flowbots","Flowbots.html#method-c-load_components","()","<p>Loads the necessary components for the application.\n<p>@return [void]\n"],["load_engtagger","Flowbots::TextTaggerProcessor","Flowbots/TextTaggerProcessor.html#method-i-load_engtagger","()","<p>Loads the EngTagger library.\n<p>@return [void]\n"],["load_existing_model","Flowbots::TopicModelProcessor","Flowbots/TopicModelProcessor.html#method-i-load_existing_model","()","<p>Loads an existing topic model from the specified path.\n<p>@return [void]\n"],["load_file_structure","Flowbots::ExceptionAgent","Flowbots/ExceptionAgent.html#method-i-load_file_structure","()","<p>Loads the file structure from the flowbots.json file.\n<p>@return [Hash] The file structure.\n"],["load_grammar","Flowbots::GrammarProcessor","Flowbots/GrammarProcessor.html#method-i-load_grammar","()","<p>Loads the grammar file and creates a parser instance.\n<p>@return [void]\n"],["load_model","Flowbots::NLPProcessor","Flowbots/NLPProcessor.html#method-i-load_model","()","<p>Loads the NLP model from the specified environment variable.\n<p>@return [void]\n"],["load_or_create_model","Flowbots::TopicModelProcessor","Flowbots/TopicModelProcessor.html#method-i-load_or_create_model","()","<p>Loads an existing topic model or creates a new one if it doesn’t exist.\n<p>@return [void]\n"],["load_state","WorkflowAgent","WorkflowAgent.html#method-i-load_state","()","<p>Loads the agent’s state from Redis.\n<p>@return [void]\n"],["load_tasks","Flowbots::Task","Flowbots/Task.html#method-c-load_tasks","()","<p>Loads all task files from the TASK_DIR directory.\n<p>@return [void]\n"],["load_workflows","Flowbots::Workflows","Flowbots/Workflows.html#method-c-load_workflows","()","<p>Class method to load all workflow files from the WORKFLOW_DIR directory.\nIt also checks for user-defined ...\n"],["log_exception","Flowbots::ExceptionHandler","Flowbots/ExceptionHandler.html#method-c-log_exception","(exception)","<p>Logs an exception to the application’s logger.\n<p>@param exception [Exception] The exception object. …\n"],["log_level","Logging","Logging.html#method-c-log_level","()","<p>Returns the default log level.\n<p>@return [Integer] The log level.\n"],["logger","Logging","Logging.html#method-i-logger","()","<p>Returns the logger for the current class and method.\n<p>@return [Logger] The logger object.\n"],["logger_for","Logging","Logging.html#method-c-logger_for","(classname, methodname)","<p>Returns the logger for the specified class and method.\n<p>@param classname [String] The name of the class. …\n"],["main","Object","Object.html#method-i-main","()",""],["main_menu","UI","UI.html#method-i-main_menu","()","<p>Displays the main menu and prompts the user for a choice.\n<p>@return [Symbol] The value of the selected choice. …\n"],["markdown_content","MarkdownYaml::Document0","MarkdownYaml/Document0.html#method-i-markdown_content","()","<p>The Markdown content section of the document.\n<p>@return [Treetop::Runtime::SyntaxNode] The Markdown content …\n"],["multi_column_box","UI::Box","UI/Box.html#method-i-multi_column_box","(data, titles)","<p>Creates a box displaying data in multiple columns with headers.\n<p>@param data [Array&lt;Array&gt;] A 2D …\n"],["new","ExceptionAgent","ExceptionAgent.html#method-c-new","(agent_role, cartridge_file)",""],["new","Flowbots::BatchProcessor","Flowbots/BatchProcessor.html#method-c-new","(input_folder_path, batch_size=10, file_types=nil)","<p>Initializes a new <code>BatchProcessor</code> instance.\n<p>@param input_folder_path [String] The path to the folder containing …\n"],["new","Flowbots::ExceptionAgent","Flowbots/ExceptionAgent.html#method-c-new","()","<p>Initializes a new instance of the ExceptionAgent class.\n<p>@return [void]\n"],["new","Flowbots::FileLoader","Flowbots/FileLoader.html#method-c-new","(file_path)","<p>Initializes a new FileLoader instance.\n<p>@param file_path [String] The path to the file to be loaded.\n<p>@return …\n"],["new","Flowbots::FlowbotError","Flowbots/FlowbotError.html#method-c-new","(message, error_code, details={})","<p>Initializes a new FlowbotError.\n<p>@param message [String] The error message.\n@param error_code [String] ...\n"],["new","Flowbots::GrammarProcessor","Flowbots/GrammarProcessor.html#method-c-new","(grammar_name)","<p>Initializes a new GrammarProcessor instance.\n<p>@param grammar_name [String] The name of the grammar to use …\n"],["new","Flowbots::NLPProcessor","Flowbots/NLPProcessor.html#method-c-new","()","<p>Initializes a new NLPProcessor instance.\n<p>@return [void]\n"],["new","Flowbots::Task","Flowbots/Task.html#method-c-new","(options={})","<p>Initializes a new Task instance.\n<p>@param options [Hash] A hash of options for the task.\n<p>@return [void]\n"],["new","Flowbots::TextProcessingWorkflow","Flowbots/TextProcessingWorkflow.html#method-c-new","(input_file_path=nil, batch_mode=false)","<p>Initializes a new TextProcessingWorkflow instance.\n<p>@param input_file_path [String, nil] The path to the …\n"],["new","Flowbots::TextProcessor","Flowbots/TextProcessor.html#method-c-new","()","<p>Initializes a new TextProcessor instance.\n<p>@return [void]\n"],["new","Flowbots::TextSegmentProcessor","Flowbots/TextSegmentProcessor.html#method-c-new","()","<p>Initializes a new TextSegmentProcessor instance.\n<p>@return [void]\n"],["new","Flowbots::TextTaggerProcessor","Flowbots/TextTaggerProcessor.html#method-c-new","()","<p>Initializes a new TextTaggerProcessor instance.\n<p>@return [void]\n"],["new","Flowbots::TextTokenizeProcessor","Flowbots/TextTokenizeProcessor.html#method-c-new","()","<p>Initializes a new TextTokenizeProcessor instance.\n<p>@return [void]\n"],["new","Flowbots::TopicModelProcessor","Flowbots/TopicModelProcessor.html#method-c-new","()","<p>Initializes a new TopicModelProcessor instance.\n<p>@return [void]\n"],["new","Flowbots::TopicModelTrainerWorkflow","Flowbots/TopicModelTrainerWorkflow.html#method-c-new","(input_folder_path=nil)","<p>Initializes a new TopicModelTrainerWorkflow instance.\n<p>@param input_folder_path [String, nil] The path …\n"],["new","Flowbots::TopicModelTrainerWorkflowtest","Flowbots/TopicModelTrainerWorkflowtest.html#method-c-new","(input_folder_path=nil)",""],["new","Flowbots::UnifiedFileProcessingPipeline","Flowbots/UnifiedFileProcessingPipeline.html#method-c-new","(input_path, batch_size: 10, file_types: %w[md markdown txt pdf json])","<p>Initializes a new UnifiedFileProcessingPipeline instance.\n<p>@param input_path [String] The path to the file …\n"],["new","Flowbots::Workflows","Flowbots/Workflows.html#method-c-new","()","<p>Initializes a new Workflows instance.\n<p>@return [void]\n"],["new","FlowiseApiClient","FlowiseApiClient.html#method-c-new","(base_url)","<p>Initializes a new FlowiseApiClient instance.\n<p>@param base_url [String] The base URL of the Flowise API …\n"],["new","MicroAgentTask","MicroAgentTask.html#method-c-new","(agent_role, cartridge_file)",""],["new","RedisConnection","RedisConnection.html#method-c-new","()","<p>Initializes a new RedisConnection instance.\n<p>@return [void]\n"],["new","Sublayer::Actions::RunTestCommandAction","Sublayer/Actions/RunTestCommandAction.html#method-c-new","(test_command:)",""],["new","Sublayer::Actions::SpeechToTextAction","Sublayer/Actions/SpeechToTextAction.html#method-c-new","(audio_data)",""],["new","Sublayer::Actions::TextToSpeechAction","Sublayer/Actions/TextToSpeechAction.html#method-c-new","(text)",""],["new","Sublayer::Actions::WriteFileAction","Sublayer/Actions/WriteFileAction.html#method-c-new","(file_contents:, file_path:)","<p>Initializes the action with the contents to write and the target file path\n@param [String] file_contents ...\n"],["new","TTY::PromptX","TTY/PromptX.html#method-c-new","(active_color:, prefix:, history: true)",""],["new","TopicModelingTask","TopicModelingTask.html#method-c-new","(model_params)",""],["new","WorkflowAgent","WorkflowAgent.html#method-c-new","(role, cartridge_file)","<p>Initializes a new WorkflowAgent instance.\n<p>@param role [String] The role of the agent.\n@param cartridge_file ...\n"],["new","WorkflowOrchestrator","WorkflowOrchestrator.html#method-c-new","()","<p>Initializes a new WorkflowOrchestrator instance.\n<p>@return [void]\n"],["newline1","MarkdownYaml::YamlFrontMatter1","MarkdownYaml/YamlFrontMatter1.html#method-i-newline1","()","<p>The first newline character after the “—” delimiter.\n<p>@return [Treetop::Runtime::SyntaxNode] …\n"],["newline2","MarkdownYaml::YamlFrontMatter1","MarkdownYaml/YamlFrontMatter1.html#method-i-newline2","()","<p>The second newline character after the “—” delimiter.\n<p>@return [Treetop::Runtime::SyntaxNode] …\n"],["notify_exception","Flowbots::ExceptionHandler","Flowbots/ExceptionHandler.html#method-c-notify_exception","(report)","<p>Notifies relevant parties about an exception.\n<p>@param report [String] The formatted exception report.\n<p>@return …\n"],["parse","Flowbots::GrammarProcessor","Flowbots/GrammarProcessor.html#method-i-parse","(text)","<p>Parses the given text using the specified grammar.\n<p>@param text [String] The text to parse.\n<p>@return [Hash, …\n"],["parse_pdf","Flowbots::FileLoader","Flowbots/FileLoader.html#method-i-parse_pdf","(file_path)","<p>Parses a PDF file and extracts its text content.\n<p>@param file_path [String] The path to the PDF file.\n<p>@return …\n"],["pending","Task","Task.html#method-c-pending","()",""],["perform_additional_tasks","Flowbots::TextProcessingWorkflow","Flowbots/TextProcessingWorkflow.html#method-i-perform_additional_tasks","(file_id)","<p>Performs additional tasks for the given file ID.\n<p>Defines the workflow for additional tasks and runs the …\n"],["pos","Cursor","Cursor.html#method-c-pos","()",""],["predict","FlowiseApiClient","FlowiseApiClient.html#method-i-predict","(chatflow_id, options={})","<p>Sends a prediction request to the Flowise API.\n<p>@param chatflow_id [String] The ID of the chatflow to use …\n"],["preprocess_file","PreprocessFileObjectTask","PreprocessFileObjectTask.html#method-i-preprocess_file","(file_object)","<p>Preprocesses the file based on its extension.\n<p>@param file_object [FileObject] The FileObject to preprocess. …\n"],["preprocess_json","PreprocessFileObjectTask","PreprocessFileObjectTask.html#method-i-preprocess_json","(file_object)","<p>Preprocesses JSON files.\n<p>@param file_object [FileObject] The FileObject to preprocess.\n<p>@return [Array( …\n"],["preprocess_markdown_yaml","PreprocessFileObjectTask","PreprocessFileObjectTask.html#method-i-preprocess_markdown_yaml","(file_object)","<p>Preprocesses Markdown files with YAML front matter.\n<p>@param file_object [FileObject] The FileObject to …\n"],["preprocess_pdf","PreprocessFileObjectTask","PreprocessFileObjectTask.html#method-i-preprocess_pdf","(file_object)","<p>Preprocesses PDF files.\n<p>@param file_object [FileObject] The FileObject to preprocess.\n<p>@return [Array(String …\n"],["preprocess_plain_text","PreprocessFileObjectTask","PreprocessFileObjectTask.html#method-i-preprocess_plain_text","(file_object)","<p>Preprocesses plain text files.\n<p>@param file_object [FileObject] The FileObject to preprocess.\n<p>@return [Array( …\n"],["print_boxes","UI::ScrollableBox","UI/ScrollableBox.html#method-c-print_boxes","(box1, box2, box_height)","<p>Prints the scrollable boxes to the console.\n<p>@param box1 [Hash] The data for the first box.\n@param box2 ...\n"],["print_navigation_info","UI::ScrollableBox","UI/ScrollableBox.html#method-c-print_navigation_info","(box1, box2)","<p>Prints navigation information for the scrollable boxes.\n<p>@param box1 [Hash] The data for the first box. …\n"],["process","Flowbots::NLPProcessor","Flowbots/NLPProcessor.html#method-i-process","(segment, options={})","<p>Processes the given segment using the loaded NLP model and returns a hash of processed tokens.\n<p>@param …\n"],["process","Flowbots::TextProcessor","Flowbots/TextProcessor.html#method-i-process","(text)","<p>Processes the given text.\n<p>This method must be implemented in subclasses.\n<p>@param text [String] The text …\n"],["process","Flowbots::TextSegmentProcessor","Flowbots/TextSegmentProcessor.html#method-i-process","(text, opts={})","<p>Segments the given text using the specified options.\n<p>@param text [String, Array] The text to be segmented. …\n"],["process","Flowbots::TextTaggerProcessor","Flowbots/TextTaggerProcessor.html#method-i-process","(text, options={})","<p>Processes the given text using the EngTagger library and returns a hash of tagged results.\n<p>@param text …\n"],["process","Flowbots::TextTokenizeProcessor","Flowbots/TextTokenizeProcessor.html#method-i-process","(text, opts={})","<p>Tokenizes the given text using the specified options.\n<p>@param text [String, Array] The text to be tokenized. …\n"],["process","Flowbots::UnifiedFileProcessingPipeline","Flowbots/UnifiedFileProcessingPipeline.html#method-i-process","()","<p>Processes the file(s) specified in the input path.\n<p>@return [void]\n"],["process","WorkflowAgent","WorkflowAgent.html#method-i-process","(input)","<p>Processes the given input using the agent’s cartridge.\n<p>@param input [String] The input to process. …\n"],["process_batch","Flowbots::BatchProcessor","Flowbots/BatchProcessor.html#method-i-process_batch","(batch_files)","<p>Processes a single batch of files.\n<p>@param batch_files [Array&lt;String&gt;] An array of file paths for …\n"],["process_batch","Flowbots::TextProcessingWorkflow","Flowbots/TextProcessingWorkflow.html#method-i-process_batch","()","<p>Processes files in batch mode.\n<p>Fetches unprocessed file IDs and performs additional tasks for each file. …\n"],["process_batch","Flowbots::TopicModelTrainerWorkflowtest","Flowbots/TopicModelTrainerWorkflowtest.html#method-i-process_batch","(batch_files)",""],["process_batch","Flowbots::UnifiedFileProcessingPipeline","Flowbots/UnifiedFileProcessingPipeline.html#method-i-process_batch","()","<p>Processes a batch of files using the batch processor.\n<p>@return [void]\n"],["process_exception","ExceptionAgent","ExceptionAgent.html#method-i-process_exception","(exception)",""],["process_exception","Flowbots::ExceptionAgent","Flowbots/ExceptionAgent.html#method-i-process_exception","(classname, exception)","<p>Processes an exception and generates a report.\n<p>@param classname [String] The name of the class where the …\n"],["process_file","Flowbots::UnifiedFileProcessingPipeline","Flowbots/UnifiedFileProcessingPipeline.html#method-i-process_file","(file_path)","<p>Processes a single file by setting the current file path in Redis and running the workflow.\n<p>@param file_path …\n"],["process_files","Flowbots::BatchProcessor","Flowbots/BatchProcessor.html#method-i-process_files","(&block)","<p>Processes the files in batches.\n<p>This method iterates through the files in the input folder, divides them …\n"],["process_files","Flowbots::TopicModelTrainerWorkflowtest","Flowbots/TopicModelTrainerWorkflowtest.html#method-i-process_files","()",""],["process_single_file","Flowbots::TextProcessingWorkflow","Flowbots/TextProcessingWorkflow.html#method-i-process_single_file","()","<p>Processes a single file.\n<p>Creates or fetches the FileObject for the input file and performs additional …\n"],["process_single_file","Flowbots::UnifiedFileProcessingPipeline","Flowbots/UnifiedFileProcessingPipeline.html#method-i-process_single_file","()","<p>Processes a single file.\n<p>@return [void]\n"],["process_text","Flowbots::CLI","Flowbots/CLI.html#method-i-process_text","(file)","<p>Processes a text file using the text processing workflow.\n<p>@param file [String] The path to the text file. …\n"],["prompt","UI","UI.html#method-i-prompt","()","<p>Returns the TTY::Prompt instance used for user interaction.\n<p>@return [TTY::Prompt] The TTY::Prompt instance. …\n"],["prompt_for_file","Flowbots::TextProcessingWorkflow","Flowbots/TextProcessingWorkflow.html#method-i-prompt_for_file","()","<p>Prompts the user to select a file using the <code>gum file</code> command.\n<p>@return [String] The path to the selected …\n"],["prompt_for_folder","Flowbots::BatchProcessor","Flowbots/BatchProcessor.html#method-i-prompt_for_folder","()","<p>Prompts the user to select a folder using the <code>gum file</code> command.\n<p>@return [String] The path to the selected …\n"],["prompt_for_folder","Flowbots::TopicModelTrainerWorkflow","Flowbots/TopicModelTrainerWorkflow.html#method-i-prompt_for_folder","()","<p>Prompts the user to select a folder using the <code>gum file</code> command.\n<p>@return [String] The path to the selected …\n"],["prompt_for_folder","Flowbots::TopicModelTrainerWorkflowtest","Flowbots/TopicModelTrainerWorkflowtest.html#method-i-prompt_for_folder","()",""],["readline","TTY::PromptX","TTY/PromptX.html#method-i-readline","(text = \"\")",""],["retrieve_file_metadata","LlmAnalysisTask","LlmAnalysisTask.html#method-i-retrieve_file_metadata","()","<p>Retrieves the file metadata from Redis.\n<p>@return [Hash] The file metadata.\n"],["retrieve_file_object","InputRetrieval","InputRetrieval.html#method-i-retrieve_file_object","()","<p>Retrieves the FileObject from Redis.\n<p>@return [FileObject, nil] The retrieved FileObject or nil if no  …\n"],["retrieve_file_path","InputRetrieval","InputRetrieval.html#method-i-retrieve_file_path","()","<p>Retrieves the file path from Redis.\n<p>@return [String] The retrieved file path.\n@raise [ArgumentError]  ...\n"],["retrieve_file_path","LoadFileObjectTask","LoadFileObjectTask.html#method-i-retrieve_file_path","()","<p>Retrieves the file path from Redis.\n<p>@return [String] The file path retrieved from Redis.\n"],["retrieve_filtered_words","TopicModelingTask","TopicModelingTask.html#method-i-retrieve_filtered_words","(textfile)","<p>Retrieves filtered words from the segments of the given FileObject.\n<p>@param textfile [FileObject] The  …\n"],["retrieve_input","AccumulateFilteredSegmentsTask","AccumulateFilteredSegmentsTask.html#method-i-retrieve_input","()","<p>Retrieves the input for the task, which is the current FileObject.\n<p>@return [FileObject] The current FileObject …\n"],["retrieve_input","DisplayResultsTask","DisplayResultsTask.html#method-i-retrieve_input","()","<p>Retrieves the input for the task, which is the current FileObject.\n<p>@return [FileObject] The current FileObject …\n"],["retrieve_input","FileLoaderTask","FileLoaderTask.html#method-i-retrieve_input","()","<p>Retrieves the input file path from Redis.\n<p>@return [String] The input file path.\n"],["retrieve_input","FilterSegmentsTask","FilterSegmentsTask.html#method-i-retrieve_input","()",""],["retrieve_input","InputRetrieval","InputRetrieval.html#method-i-retrieve_input","()","<p>Retrieves the input data for a task.\n<p>This method first attempts to retrieve a FileObject from Redis.\n ...\n"],["retrieve_input","LlmAnalysisTask","LlmAnalysisTask.html#method-i-retrieve_input","()",""],["retrieve_input","LoadTextFilesTask","LoadTextFilesTask.html#method-i-retrieve_input","()","<p>Retrieves the input file path from Redis.\n<p>@return [String] The input file path.\n"],["retrieve_input","NlpAnalysisTask","NlpAnalysisTask.html#method-i-retrieve_input","()","<p>Retrieves the input for the task, which is the current FileObject.\n<p>@return [FileObject] The current FileObject …\n"],["retrieve_input","Task","Task.html#method-i-retrieve_input","()",""],["retrieve_input","TextSegmentTask","TextSegmentTask.html#method-i-retrieve_input","()","<p>Retrieves the input for the task, which is the current FileObject.\n<p>@return [FileObject] The current FileObject …\n"],["retrieve_input","TextTaggerTask","TextTaggerTask.html#method-i-retrieve_input","()","<p>Retrieves the input for the task, which is the current FileObject.\n<p>@return [FileObject] The current FileObject …\n"],["retrieve_input","TokenizeSegmentsTask","TokenizeSegmentsTask.html#method-i-retrieve_input","()","<p>Retrieves the input for the task, which is the current FileObject.\n<p>@return [FileObject] The current FileObject …\n"],["retrieve_input","TopicModelingTask","TopicModelingTask.html#method-i-retrieve_input","()","<p>Retrieves the input for the task, which is the current FileObject.\n<p>@return [FileObject] The current FileObject …\n"],["retrieve_nlp_result","LlmAnalysisTask","LlmAnalysisTask.html#method-i-retrieve_nlp_result","(textfile)","<p>Retrieves the NLP results for the segments of the Textfile.\n<p>@param textfile [Textfile] The Textfile object. …\n"],["retrieve_segment_texts","FileObject","FileObject.html#method-i-retrieve_segment_texts","()",""],["retrieve_segments","FileObject","FileObject.html#method-i-retrieve_segments","()",""],["retrieve_word_texts","FileObject","FileObject.html#method-i-retrieve_word_texts","()",""],["retrieve_word_texts","Segment","Segment.html#method-i-retrieve_word_texts","()",""],["retrieve_words","FileObject","FileObject.html#method-i-retrieve_words","()",""],["retrieve_words","Segment","Segment.html#method-i-retrieve_words","()",""],["root","MarkdownYaml","MarkdownYaml.html#method-i-root","()","<p>The root node of the grammar.\n<p>@return [Treetop::Runtime::SyntaxNode] The root node of the grammar.\n"],["run","Flowbots::TextProcessingWorkflow","Flowbots/TextProcessingWorkflow.html#method-i-run","()","<p>Runs the text processing workflow.\n<p>Sets up the workflow, processes the file(s), and performs additional …\n"],["run","Flowbots::TopicModelTrainerWorkflow","Flowbots/TopicModelTrainerWorkflow.html#method-i-run","()","<p>Runs the topic model trainer workflow.\n<p>Sets up the workflow, processes the files, and trains the topic …\n"],["run","Flowbots::TopicModelTrainerWorkflowtest","Flowbots/TopicModelTrainerWorkflowtest.html#method-i-run","()",""],["run","Flowbots::Workflows","Flowbots/Workflows.html#method-i-run","(workflow_name)","<p>Runs the specified workflow.\n<p>@param workflow_name [String] The name of the workflow to run.\n<p>@return [void] …\n"],["run_workflow","WorkflowOrchestrator","WorkflowOrchestrator.html#method-i-run_workflow","()","<p>Runs the defined workflow.\n<p>This method initiates the workflow execution, managing the lifecycle of tasks …\n"],["save_model","Flowbots::TopicModelProcessor","Flowbots/TopicModelProcessor.html#method-i-save_model","()","<p>Saves the topic model to the specified path.\n<p>@return [void]\n"],["save_state","WorkflowAgent","WorkflowAgent.html#method-i-save_state","()","<p>Saves the agent’s state to Redis.\n<p>@return [void]\n"],["say","UI","UI.html#method-i-say","(type, statement)","<p>Displays a message to the user with the specified type and logs it.\n<p>@param type [Symbol] The type of message …\n"],["segment_array","Flowbots::TextSegmentProcessor","Flowbots/TextSegmentProcessor.html#method-i-segment_array","()","<p>Segments an array of text.\n<p>@return [Array] An array of segments.\n"],["segment_string","Flowbots::TextSegmentProcessor","Flowbots/TextSegmentProcessor.html#method-i-segment_string","(txt)","<p>Segments a single string.\n<p>@param txt [String] The text to be segmented.\n<p>@return [Array] An array of segments. …\n"],["select_workflow","Flowbots::Workflows","Flowbots/Workflows.html#method-i-select_workflow","(workflows)","<p>Prompts the user to select a workflow from the list of available workflows.\n<p>@param workflows [Array&lt;Array( …\n"],["set","RedisKeys","RedisKeys.html#method-c-set","(key, value)","<p>Sets the value associated with the given key in Redis.\n<p>@param key [String] The Redis key.\n@param value ...\n"],["setup_redis","Flowbots","Flowbots.html#method-c-setup_redis","()","<p>Sets up the Redis connection for Ohm.\n<p>@return [void]\n@raise [Ohm::Error] If there is an error connecting ...\n"],["setup_workflow","Flowbots::TopicModelTrainerWorkflowtest","Flowbots/TopicModelTrainerWorkflowtest.html#method-i-setup_workflow","()",""],["setup_workflow","Flowbots::UnifiedFileProcessingPipeline","Flowbots/UnifiedFileProcessingPipeline.html#method-i-setup_workflow","()","<p>Sets up the workflow by defining the task graph and adding agents to the orchestrator.\n<p>@return [void] …\n"],["shutdown","Flowbots","Flowbots.html#method-c-shutdown","()","<p>Shuts down the Flowbots application.\n<p>@return [void]\n"],["side_by_side_boxes","UI::ScrollableBox","UI/ScrollableBox.html#method-i-side_by_side_boxes","(text1, text2, title1: \"Box 1\", title2: \"Box 2\")","<p>Creates and displays two scrollable boxes side-by-side for comparison.\n<p>@param text1 [String] The text …\n"],["spinner","UI","UI.html#method-i-spinner","(text)","<p>Creates and returns a TTY::Spinner instance with the specified text.\n<p>@param text [String] The text to …\n"],["splat_sort","API","API.html#method-i-splat_sort","(splat_vals)","<p>… (Add routes for other object buckets and their attributes) …\n"],["stop_running_workflows","Flowbots","Flowbots.html#method-c-stop_running_workflows","()","<p>Stops any running workflows.\n<p>@return [void]\n"],["store_FileObject_id","FileLoaderTask","FileLoaderTask.html#method-i-store_FileObject_id","(id)","<p>Stores the FileObject ID in Redis.\n<p>@param id [Integer] The ID of the FileObject.\n<p>@return [void]\n"],["store_analysis_result","LlmAnalysisTask","LlmAnalysisTask.html#method-i-store_analysis_result","(textfile, result)","<p>Stores the analysis result in the Textfile.\n<p>@param textfile [Textfile] The Textfile object.\n@param result ...\n"],["store_exception_report","ExceptionAgent","ExceptionAgent.html#method-i-store_exception_report","(report)",""],["store_file_data","Flowbots::FileLoader","Flowbots/FileLoader.html#method-i-store_file_data","(file_path, extracted_text)","<p>Stores the file data in the database.\n<p>@param file_path [String] The path to the file.\n@param extracted_text ...\n"],["store_file_object_id","LoadFileObjectTask","LoadFileObjectTask.html#method-i-store_file_object_id","(id)","<p>Stores the FileObject ID in Redis.\n<p>@param id [Integer] The ID of the FileObject to store.\n@return [void] ...\n"],["store_preprocessed_data","PreprocessFileObjectTask","PreprocessFileObjectTask.html#method-i-store_preprocessed_data","(content, metadata)","<p>Stores the preprocessed content and metadata in the FileObject.\n<p>@param content [String] The preprocessed …\n"],["store_result","MicroAgentTask","MicroAgentTask.html#method-i-store_result","(result)",""],["store_result","TextTaggerTask","TextTaggerTask.html#method-i-store_result","(file_object, result, main_topics, speech_acts, transitivity)","<p>Stores the tagging results in the FileObject.\n<p>@param file_object [FileObject] The FileObject to store …\n"],["store_segments","TextSegmentTask","TextSegmentTask.html#method-i-store_segments","(textfile, segments)","<p>Stores the given segments in the given FileObject.\n<p>@param textfile [FileObject] The FileObject to store …\n"],["store_textfile_id","LoadTextFilesTask","LoadTextFilesTask.html#method-i-store_textfile_id","(id)","<p>Stores the Textfile ID in Redis.\n<p>@param id [Integer] The ID of the Textfile.\n<p>@return [void]\n"],["store_topic_result","TopicModelingTask","TopicModelingTask.html#method-i-store_topic_result","(textfile, result)","<p>Stores the topic modeling results in the given FileObject.\n<p>Extracts unique words from the topic results, …\n"],["store_topics","TopicModelingTask","TopicModelingTask.html#method-i-store_topics","(topics)",""],["tokenize_array","Flowbots::TextTokenizeProcessor","Flowbots/TextTokenizeProcessor.html#method-i-tokenize_array","()","<p>Tokenizes an array of strings.\n<p>@return [Array] An array of tokens.\n"],["tokenize_string","Flowbots::TextTokenizeProcessor","Flowbots/TextTokenizeProcessor.html#method-i-tokenize_string","(str)","<p>Tokenizes a single string.\n<p>@param str [String] The string to be tokenized.\n<p>@return [Array] An array of …\n"],["train_model","Flowbots::TopicModelProcessor","Flowbots/TopicModelProcessor.html#method-i-train_model","(documents, iterations=100)","<p>Trains a topic model using the provided documents.\n<p>@param documents [Array] An array of documents to train …\n"],["train_topic_model","Flowbots::CLI","Flowbots/CLI.html#method-i-train_topic_model","(folder)","<p>Trains a topic model using text files in the specified folder.\n<p>@param folder [String] The path to the …\n"],["train_topic_model","Flowbots::TopicModelTrainerWorkflow","Flowbots/TopicModelTrainerWorkflow.html#method-i-train_topic_model","()","<p>Trains the topic model using the filtered segments from the processed files.\n<p>Retrieves the filtered segments …\n"],["train_topic_model","Flowbots::TopicModelTrainerWorkflowtest","Flowbots/TopicModelTrainerWorkflowtest.html#method-i-train_topic_model","()",""],["update_file_object","AccumulateFilteredSegmentsTask","AccumulateFilteredSegmentsTask.html#method-i-update_file_object","(cleaned_segments)","<p>Updates the FileObject with the given cleaned segments.\n<p>@param cleaned_segments [Array&lt;Array&lt;String&gt;&gt;] …\n"],["update_segment_with_nlp_data","NlpAnalysisTask","NlpAnalysisTask.html#method-i-update_segment_with_nlp_data","(segment, processed_tokens, lemma_counts)","<p>Updates a segment with NLP data.\n<p>Extracts relevant NLP information from the processed tokens and updates …\n"],["update_state","WorkflowAgent","WorkflowAgent.html#method-i-update_state","(response)","<p>Updates the agent’s state with the latest response.\n<p>@param response [String] The agent’s response. …\n"],["upsert_document","FlowiseApiClient","FlowiseApiClient.html#method-i-upsert_document","(chatflow_id, file_path, local_ai_config={})","<p>Sends a document upsert request to the Flowise API.\n<p>@param chatflow_id [String] The ID of the chatflow …\n"],["version","Flowbots::CLI","Flowbots/CLI.html#method-i-version","()","<p>Displays the Flowbots version and Ruby environment information.\n<p>@return [void]\n"],["workflows","Flowbots::CLI","Flowbots/CLI.html#method-i-workflows","()","<p>Lists available workflows, allows the user to select one, and runs it.\n<p>@return [void]\n"],["write_markdown","LlmAnalysisTask","LlmAnalysisTask.html#method-i-write_markdown","(textfile, analysis_result)","<p>Writes the LLM analysis result to a Markdown file.\n<p>@param textfile [Textfile] The Textfile object.\n@param ...\n"],["write_markdown_report","Flowbots::ExceptionAgent","Flowbots/ExceptionAgent.html#method-i-write_markdown_report","(report, exception_details)","<p>Writes the exception report to a markdown file.\n<p>@param report [String] The exception report.\n@param exception_details ...\n"],["write_markdown_report","LlmAnalysisTask","LlmAnalysisTask.html#method-i-write_markdown_report","(result)","<p>Writes the exception report to a markdown file.\n<p>@param report [String] The exception report.\n@param exception_details ...\n"],["yaml_front_matter","MarkdownYaml::Document0","MarkdownYaml/Document0.html#method-i-yaml_front_matter","()","<p>The YAML front matter section of the document.\n<p>@return [Treetop::Runtime::SyntaxNode] The YAML front matter …\n"],["LICENSE","","LICENSE.html","","<p>The MIT License (MIT)\n<p>Copyright © 2024 Robert Pannick\n<p>Permission is hereby granted, free of charge, to …\n"],["README","","README_md.html","","<p>Flowbots\n<p>Flowbots is an advanced text processing and analysis system that combines the power of nano-bots, …\n"]]}}